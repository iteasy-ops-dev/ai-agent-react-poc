from openai import OpenAI
import requests
import json
import time
import os
from typing import List, Dict, Any, Optional, Union
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# ëª¨ë¸ëª…ê³¼ ì‚¬ìš©í•  ë¼ì´ë¸ŒëŸ¬ë¦¬ ë§¤í•‘
MODEL_LIBRARY_MAPPINGS = {
    # OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ëª¨ë¸ë“¤
    "gpt-4": "openai",           # ì‹¤ì œ OpenAI API
    "gpt-4": "openai",           # ì‹¤ì œ OpenAI API
    "gpt-3": "openai",           # ì‹¤ì œ OpenAI API  
    "gpt-oss": "openai",         # Ollamaì˜ OpenAI í˜¸í™˜ API
    "llama": "openai",           # Ollamaì˜ OpenAI í˜¸í™˜ API
    "mixtral": "openai",         # Ollamaì˜ OpenAI í˜¸í™˜ API
    "codellama": "openai",       # Ollamaì˜ OpenAI í˜¸í™˜ API
    
    # ê° ëª¨ë¸ì˜ ê³µì‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ (í–¥í›„ í™•ì¥ìš©)
    "qwen": "dashscope",              # Alibaba Qwen ê³µì‹
    "claude": "anthropic",            # Anthropic ê³µì‹
    "gemini": "google-generativeai",  # Google ê³µì‹
    "cohere": "cohere"                # Cohere ê³µì‹
}

# ë¼ì´ë¸ŒëŸ¬ë¦¬ë³„ ì—”ë“œí¬ì¸íŠ¸ íƒ€ì…
LIBRARY_ENDPOINT_TYPES = {
    "openai": ["local", "remote"],    # ë¡œì»¬(Ollama) ë˜ëŠ” ì›ê²©(OpenAI)
    "dashscope": ["remote"],          # ì›ê²©ë§Œ
    "anthropic": ["remote"],          # ì›ê²©ë§Œ  
    "google-generativeai": ["remote"], # ì›ê²©ë§Œ
    "cohere": ["remote"]              # ì›ê²©ë§Œ
}


class LLMClient:
    """
    LLM í†µì‹ ì„ ìœ„í•œ í´ë¼ì´ì–¸íŠ¸ í´ë˜ìŠ¤
    ëª¨ë¸ëª…ì— ë”°ë¼ ì ì ˆí•œ ê³µì‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë™ì ìœ¼ë¡œ ì„ íƒí•˜ì—¬ í†µì‹ 
    """
    
    def __init__(self, endpoint: str = "http://localhost:11434", model: str = "gpt-oss:20b"):
        """
        LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        ëª¨ë¸ëª…ì— ë”°ë¼ ì ì ˆí•œ ê³µì‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë™ì ìœ¼ë¡œ ì„ íƒ
        
        Args:
            endpoint (str): LLM ì„œë²„ ì£¼ì†Œ (ê¸°ë³¸ê°’: localhost:11434)
            model (str): ì‚¬ìš©í•  ëª¨ë¸ëª… (ê¸°ë³¸ê°’: gpt-oss:20b)
        """
        self.endpoint = endpoint.rstrip('/')
        self.model = model
        self.library_type = self._get_library_for_model(model)
        
        # ëª¨ë¸ì— ë”°ë¼ ì ì ˆí•œ í´ë¼ì´ì–¸íŠ¸ ë™ì  ìƒì„±
        self.client = self._create_client(model, endpoint)
        
        # ì§ì ‘ HTTP ìš”ì²­ìš© (ëª¨ë¸ ë¦¬ìŠ¤íŠ¸, í—¬ìŠ¤ì²´í¬ ë“±)
        self.session = requests.Session()
        self.session.timeout = 30
    
    def _get_library_for_model(self, model: str) -> str:
        """
        ëª¨ë¸ëª…ì—ì„œ ì‚¬ìš©í•  ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²°ì •
        
        Args:
            model (str): ëª¨ë¸ëª…
            
        Returns:
            str: ì‚¬ìš©í•  ë¼ì´ë¸ŒëŸ¬ë¦¬ëª…
        """
        model_lower = model.lower()
        
        # ëª¨ë¸ëª… íŒ¨í„´ ë§¤ì¹­
        for model_pattern, library in MODEL_LIBRARY_MAPPINGS.items():
            if model_pattern in model_lower:
                return library
        
        # ê¸°ë³¸ê°’: OpenAI ë¼ì´ë¸ŒëŸ¬ë¦¬ (ê°€ì¥ í˜¸í™˜ì„±ì´ ë†’ìŒ)
        return "openai"
    
    def _create_client(self, model: str, endpoint: str):
        """
        ëª¨ë¸ê³¼ ì—”ë“œí¬ì¸íŠ¸ì— ë”°ë¼ ì ì ˆí•œ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        
        Args:
            model (str): ëª¨ë¸ëª…
            endpoint (str): ì—”ë“œí¬ì¸íŠ¸
            
        Returns:
            í´ë¼ì´ì–¸íŠ¸ ì¸ìŠ¤í„´ìŠ¤
        """
        library = self._get_library_for_model(model)
        
        if library == "openai":
            return self._create_openai_client(model, endpoint)
        elif library == "dashscope":
            return self._create_dashscope_client()
        elif library == "anthropic":
            return self._create_anthropic_client()
        elif library == "google-generativeai":
            return self._create_google_client()
        elif library == "cohere":
            return self._create_cohere_client()
        else:
            # ê¸°ë³¸ê°’: OpenAI í´ë¼ì´ì–¸íŠ¸
            return self._create_openai_client(model, endpoint)
    
    def _create_openai_client(self, model: str, endpoint: str):
        """OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
        model_lower = model.lower()
        
        # ë¡œì»¬ ëª¨ë¸ (Ollama)ì¸ì§€ í™•ì¸
        local_models = ["gpt-oss", "llama", "mixtral", "codellama", "iteasy-gpt"]
        is_local = any(local_model in model_lower for local_model in local_models)
        
        if is_local:
            # Ollama ì„œë²„ ì‚¬ìš©
            return OpenAI(
                base_url=f"{endpoint}/v1",
                api_key="dummy"  # OllamaëŠ” API í‚¤ ë¶ˆí•„ìš”
            )
        else:
            # ì‹¤ì œ OpenAI API ì‚¬ìš©
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise ValueError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return OpenAI(api_key=api_key)
    
    def _create_dashscope_client(self):
        """Qwen/DashScope í´ë¼ì´ì–¸íŠ¸ ìƒì„± (í–¥í›„ êµ¬í˜„)"""
        raise NotImplementedError("DashScope í´ë¼ì´ì–¸íŠ¸ëŠ” ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    def _create_anthropic_client(self):
        """Anthropic í´ë¼ì´ì–¸íŠ¸ ìƒì„± (í–¥í›„ êµ¬í˜„)"""
        raise NotImplementedError("Anthropic í´ë¼ì´ì–¸íŠ¸ëŠ” ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    def _create_google_client(self):
        """Google AI í´ë¼ì´ì–¸íŠ¸ ìƒì„± (í–¥í›„ êµ¬í˜„)"""
        raise NotImplementedError("Google AI í´ë¼ì´ì–¸íŠ¸ëŠ” ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    def _create_cohere_client(self):
        """Cohere í´ë¼ì´ì–¸íŠ¸ ìƒì„± (í–¥í›„ êµ¬í˜„)"""
        raise NotImplementedError("Cohere í´ë¼ì´ì–¸íŠ¸ëŠ” ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    def health_check(self) -> Dict[str, Any]:
        """
        ì„œë²„ ì—°ê²° ìƒíƒœ í™•ì¸
        
        Returns:
            Dict: í—¬ìŠ¤ì²´í¬ ê²°ê³¼ (status, response_time, error ë“±)
        """
        start_time = time.time()
        
        try:
            # Ollama í—¬ìŠ¤ì²´í¬ìš© ì—”ë“œí¬ì¸íŠ¸ë“¤ ì‹œë„
            health_endpoints = [
                f"{self.endpoint}/api/tags",      # ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ (Ollama)
                f"{self.endpoint}/api/version",   # ë²„ì „ ì •ë³´ (Ollama)
                f"{self.endpoint}/v1/models"      # OpenAI í˜¸í™˜ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸
            ]
            
            last_error = None
            
            for url in health_endpoints:
                try:
                    response = self.session.get(url, timeout=10)
                    response_time = time.time() - start_time
                    
                    if response.status_code == 200:
                        return {
                            "status": "healthy",
                            "response_time": round(response_time, 3),
                            "endpoint": self.endpoint,
                            "model": self.model,
                            "server_info": response.json() if response.content else None
                        }
                        
                except requests.exceptions.RequestException as e:
                    last_error = str(e)
                    continue
            
            # ëª¨ë“  ì—”ë“œí¬ì¸íŠ¸ ì‹¤íŒ¨
            response_time = time.time() - start_time
            return {
                "status": "unhealthy",
                "response_time": round(response_time, 3),
                "endpoint": self.endpoint,
                "model": self.model,
                "error": last_error or "All health check endpoints failed"
            }
            
        except Exception as e:
            response_time = time.time() - start_time
            return {
                "status": "error",
                "response_time": round(response_time, 3),
                "endpoint": self.endpoint,
                "model": self.model,
                "error": str(e)
            }
    
    def get_models(self) -> Dict[str, Any]:
        """
        ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
        
        Returns:
            Dict: ëª¨ë¸ ëª©ë¡ê³¼ ë©”íƒ€ë°ì´í„°
        """
        try:
            # Ollama APIë¡œ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ ì‹œë„
            ollama_url = f"{self.endpoint}/api/tags"
            
            try:
                response = self.session.get(ollama_url, timeout=10)
                if response.status_code == 200:
                    data = response.json()
                    
                    models = []
                    for model in data.get("models", []):
                        models.append({
                            "name": model.get("name", ""),
                            "size": model.get("size", 0),
                            "modified_at": model.get("modified_at", ""),
                            "digest": model.get("digest", ""),
                            "details": model.get("details", {})
                        })
                    
                    return {
                        "success": True,
                        "models": models,
                        "count": len(models),
                        "endpoint": self.endpoint
                    }
                    
            except (requests.exceptions.RequestException, json.JSONDecodeError):
                pass
            
            # OpenAI í˜¸í™˜ APIë¡œ ì‹œë„
            try:
                models_response = self.client.models.list()
                
                models = []
                for model in models_response.data:
                    models.append({
                        "name": model.id,
                        "created": model.created,
                        "owned_by": model.owned_by,
                        "object": model.object
                    })
                
                return {
                    "success": True,
                    "models": models,
                    "count": len(models),
                    "endpoint": self.endpoint
                }
                
            except Exception:
                pass
            
            # ëª¨ë“  ë°©ë²• ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‘ë‹µ
            return {
                "success": False,
                "models": [],
                "count": 0,
                "endpoint": self.endpoint,
                "error": "Could not retrieve model list from endpoint"
            }
            
        except Exception as e:
            return {
                "success": False,
                "models": [],
                "count": 0,
                "endpoint": self.endpoint,
                "error": str(e)
            }
    
    def chat_completion(
        self, 
        messages: Union[str, List[Dict[str, str]]], 
        tools: Optional[List[Dict]] = None,
        stream: bool = False,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        ì±„íŒ… ì™„ë£Œ ìš”ì²­
        ì´ˆê¸°í™”ì‹œ ì§€ì •í•œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í†µì‹ 
        
        Args:
            messages: ë©”ì‹œì§€ (ë¬¸ìì—´ ë˜ëŠ” ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸)
            tools: Function callingìš© ë„êµ¬ ì •ì˜
            stream: ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì—¬ë¶€
            temperature: ì‘ë‹µ ì°½ì˜ì„± (0.0-2.0)
            max_tokens: ìµœëŒ€ í† í° ìˆ˜
            **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„°
            
        Returns:
            Dict: ì‘ë‹µ ê²°ê³¼
        """
        try:
            # ë©”ì‹œì§€ í˜•ì‹ ì •ê·œí™”
            if isinstance(messages, str):
                messages = [{"role": "user", "content": messages}]
            
            # ë¼ì´ë¸ŒëŸ¬ë¦¬ íƒ€ì…ì— ë”°ë¼ ë‹¤ë¥¸ ì²˜ë¦¬
            if self.library_type == "openai":
                return self._openai_chat_completion(messages, tools, stream, temperature, max_tokens, **kwargs)
            elif self.library_type == "dashscope":
                return self._dashscope_chat_completion(messages, tools, stream, temperature, max_tokens, **kwargs)
            elif self.library_type == "anthropic":
                return self._anthropic_chat_completion(messages, tools, stream, temperature, max_tokens, **kwargs)
            else:
                # ê¸°ë³¸ê°’: OpenAI ë°©ì‹
                return self._openai_chat_completion(messages, tools, stream, temperature, max_tokens, **kwargs)
                
        except Exception as e:
            return {
                "success": False,
                "response": f"Error during chat completion: {str(e)}",
                "model": self.model,
                "endpoint": self.endpoint,
                "library": self.library_type,
                "error": str(e)
            }
    
    def _openai_chat_completion(self, messages, tools, stream, temperature, max_tokens, **kwargs):
        """OpenAI ë°©ì‹ì˜ ì±„íŒ… ì™„ë£Œ"""

        # TODO: modelì— ë”°ë¼ ì ì ˆí•œ íŒŒë¼ë¯¸í„° ì¡°ì • í•„ìš” (ì˜ˆ: gpt-4o, gpt-3.5-turbo ë“±)
        # gpt-5 ì‹œë¦¬ì¦ˆì˜ ì¶”ë¡  ëª¨ë¸ì¼ ê²½ìš°, temperatureëŠ” ë°˜ë“œì‹œ 1ë¡œ ê³ ì •
        if "gpt-5" in self.model.lower():
            temperature = 1.0
        # ìš”ì²­ íŒŒë¼ë¯¸í„° êµ¬ì„±
        request_params = {
            "model": self.model,
            "messages": messages,
            "temperature": temperature,
            "stream": stream
        }
        
        if max_tokens:
            request_params["max_tokens"] = max_tokens
            
        if tools:
            request_params["tools"] = tools
            
        # ì¶”ê°€ íŒŒë¼ë¯¸í„° ë³‘í•©
        request_params.update(kwargs)
        
        # API ìš”ì²­
        response = self.client.chat.completions.create(**request_params)

        # ì¶”ë¡  ì •ë³´ ì¶”ì¶œ
        reasoning_content = None
        if hasattr(response.choices[0].message, 'reasoning') and response.choices[0].message.reasoning:
            reasoning_content = response.choices[0].message.reasoning
            # print("-" * 30)
            # print("ğŸ“Š ëª¨ë¸ ì‘ë‹µ ì •ë³´:") 
            # print(f"ğŸ§  ì¶”ë¡  ê³¼ì •: {reasoning_content}")
            
        if stream:
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
            return {
                "success": True,
                "stream": response,
                "model": self.model,
                "endpoint": self.endpoint,
                "library": self.library_type
            }
        else:
            # ì¼ë°˜ ì‘ë‹µ
            return {
                "success": True,
                "response": response.choices[0].message.content,
                "message": response.choices[0].message,
                "reasoning": reasoning_content,  # ì¶”ë¡  ì •ë³´ ì¶”ê°€
                "usage": response.usage.model_dump() if response.usage else None,
                "model": self.model,
                "endpoint": self.endpoint,
                "library": self.library_type,
                "function_result": None,  # Function calling ê²°ê³¼ (í•„ìš”ì‹œ ì²˜ë¦¬)
                "finish_reason": response.choices[0].finish_reason
            }
    
    def _dashscope_chat_completion(self, messages, tools, stream, temperature, max_tokens, **kwargs):
        """DashScope ë°©ì‹ì˜ ì±„íŒ… ì™„ë£Œ (í–¥í›„ êµ¬í˜„)"""
        raise NotImplementedError("DashScope ì±„íŒ… ì™„ë£ŒëŠ” ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    def _anthropic_chat_completion(self, messages, tools, stream, temperature, max_tokens, **kwargs):
        """Anthropic ë°©ì‹ì˜ ì±„íŒ… ì™„ë£Œ (í–¥í›„ êµ¬í˜„)"""
        raise NotImplementedError("Anthropic ì±„íŒ… ì™„ë£ŒëŠ” ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    def __str__(self) -> str:
        """ë¬¸ìì—´ í‘œí˜„"""
        return f"LLMClient(endpoint='{self.endpoint}', model='{self.model}', library='{self.library_type}')"
    
    def __repr__(self) -> str:
        """ê°œë°œììš© í‘œí˜„"""
        return self.__str__()